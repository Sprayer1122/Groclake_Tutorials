{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sprayer1122/Groclake_Tutorials/blob/main/Groclake_Datalake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groclake DataLake Colab Notebook\n",
        "\n",
        "DataLake is a centralized platform for storing, managing, and retrieving large volumes of structured and unstructured data, such as documents, files, and metadata. It supports efficient data operations and enables advanced processing, retrieval, and analysis, making it ideal for AI/ML applications and businesses needing to manage complex datasets at scale.\n",
        "\n"
      ],
      "metadata": {
        "id": "lYSZAPHT4x5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 1: Install Required Library**\n",
        "Install the Groclake library to interact with its APIs and manage DataLakes"
      ],
      "metadata": {
        "id": "xrCI6i2C46PX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzBPhMlHxpvX"
      },
      "outputs": [],
      "source": [
        "!pip install groclake\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2: Import Required Modules and Set up env variables**"
      ],
      "metadata": {
        "id": "L95cTz325Cgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groclake.cataloglake import Cataloglake\n",
        "from groclake.modellake import Modellake\n",
        "from groclake.datalake import Datalake\n",
        "from groclake.vectorlake import Vectorlake\n",
        "\n",
        "# Environment variable setup\n",
        "GROCLAKE_API_KEY = 'your_groclake_api_key'\n",
        "GROCLAKE_ACCOUNT_ID = 'your_groclake_account_id'\n",
        "\n",
        "os.environ['GROCLAKE_API_KEY'] = GROCLAKE_API_KEY\n",
        "os.environ['GROCLAKE_ACCOUNT_ID'] = GROCLAKE_ACCOUNT_ID\n",
        "\n",
        "# Initialize Groclake catalog instance\n",
        "model_lake = Modellake()\n",
        "data_lake = Datalake()\n",
        "vector_lake = Vectorlake()"
      ],
      "metadata": {
        "id": "lfciMmww5Erh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3: Initialize Groclake Instances**"
      ],
      "metadata": {
        "id": "Npj5fJ8j5WPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances for DataLake\n",
        "data_lake = Datalake()"
      ],
      "metadata": {
        "id": "H3YLO0Gf5axr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 4: Create a New DataLake**\n",
        "Create a new DataLake, which serves as a storage and processing environment for data"
      ],
      "metadata": {
        "id": "aEBLakbB5l8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    data_create = data_lake.create()\n",
        "    print(\"DataLake Created Successfully:\", data_create)\n",
        "    # Store the DataLake ID for further operations\n",
        "    datalake_id = data_create[\"datalake_id\"]\n",
        "except Exception as e:\n",
        "    print(\"Error creating DataLake:\", str(e))"
      ],
      "metadata": {
        "id": "vVElAoW-5pjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 5: Push a Document to the DataLake**\n",
        "Push a document (in this case, a URL) to the created DataLake for storage and processing\n",
        "\n",
        "Instructions to Generate a Publicly Accessible Link:\n",
        "\n",
        "Google Drive:\n",
        "\n",
        "1. Upload your file (e.g., a PDF) to Google Drive.\n",
        "2. Right-click on the uploaded file and select \"Get Link\".\n",
        "3. Change the permissions:\n",
        "4. Click on the drop-down next to \"Restricted\".\n",
        "5. Select \"Anyone with the link\".\n",
        "6. Ensure the access level is set to \"Viewer\".\n",
        "7. Copy the generated link.\n",
        "8. Convert the shared link to a direct download link:\n",
        "    \n",
        "    Original link:\n",
        "    https://drive.google.com/file/d/1cOYyJ5RuTjLph6Hjx_tAhGN_xH74tBtr/view\n",
        "\n",
        "    Direct download link:\n",
        "    https://drive.google.com/uc?export=download&id=1cOYyJ5RuTjLph6Hjx_tAhGN_xH74tBtr.\n",
        "\n",
        "Dropbox:\n",
        "\n",
        "1. Upload your file to Dropbox.\n",
        "2. Right-click on the uploaded file and select \"Copy Dropbox Link\".\n",
        "3. Ensure the link is set to public access.\n",
        "4. Modify the link if needed (replace dl=0 with dl=1 at the end of the URL for direct download).\n",
        "\n",
        "OneDrive:\n",
        "\n",
        "1. Upload your file to OneDrive.\n",
        "2. Right-click on the uploaded file and select \"Share\".\n",
        "3. Choose \"Anyone with the link\" and set the access level to \"View\".\n",
        "4. Copy the link.\n",
        "\n",
        "Example Using Google Drive:\n",
        "Original Link:\n",
        "https://drive.google.com/file/d/1cOYyJ5RuTjLph6Hjx_tAhGN_xH74tBtr/view\n",
        "\n",
        "Converted Direct Download Link:\n",
        "https://drive.google.com/uc?export=download&id=1cOYyJ5RuTjLph6Hjx_tAhGN_xH74tBtr\n",
        "\n",
        "Make sure the link is a download link. You can convert your link to a download link by making minor tweaks to the URL itself. Use ChatGPT for assistance."
      ],
      "metadata": {
        "id": "haF9bHtH5sE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    payload_push = {\n",
        "        \"datalake_id\": datalake_id,  # Specify the target DataLake\n",
        "        \"document_type\": \"url\",    # Document type can be 'url', 'text', etc.\n",
        "        \"document_data\": \"https://plotch.ai/upload/plotch.pdf\"  # URL of the document to push\n",
        "    }\n",
        "\n",
        "    # https://drive.google.com/uc?export=download&id=1cOYyJ5RuTjLph6Hjx_tAhGN_xH74tBtr\n",
        "\n",
        "    # Push the document to the DataLake\n",
        "    data_push = data_lake.push(payload_push)\n",
        "    print(\"Response from push:\", data_push)\n",
        "\n",
        "    # Extract the document_id from the response, which will be used for retrieval\n",
        "    if \"document_id\" in data_push:\n",
        "        document_id = data_push[\"document_id\"]\n",
        "        print(\"Document ID:\", document_id)\n",
        "    else:\n",
        "        print(\"Error: 'document_id' not found in the response.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error pushing document:\", str(e))"
      ],
      "metadata": {
        "id": "mevRMq-p5yf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The document_id uniquely identifies the pushed document and links it to the DataLake"
      ],
      "metadata": {
        "id": "M3CUIRE854zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 6: Fetch the Document in Chunks**\n",
        "Define the payload to fetch the document in chunked format"
      ],
      "metadata": {
        "id": "VwN0qWiC57x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload_fetch = {\n",
        "    \"document_id\": document_id,  # Specify the document to fetch\n",
        "    \"datalake_id\": datalake_id,  # Specify the DataLake where the document resides\n",
        "    \"fetch_format\": \"chunk\",   # Fetching in chunks allows partial retrieval for large files\n",
        "    \"chunk_size\": \"500\"         # Define the size of each chunk (in bytes or characters)\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Fetch the document from the DataLake\n",
        "    data_fetch = data_lake.fetch(payload_fetch)\n",
        "    print(\"Document Fetched Successfully:\\n\", data_fetch)\n",
        "\n",
        "    # When fetching in chunks, the document is divided into manageable pieces for processing\n",
        "except Exception as e:\n",
        "    print(\"Error fetching document:\", str(e))"
      ],
      "metadata": {
        "id": "SqEeheBj6Cmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 7: Display the Fetched Data**\n",
        "Iterate over and display chunks of the fetched document for easy readability"
      ],
      "metadata": {
        "id": "r_j2CQ8G6LU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    document_chunks = data_fetch.get(\"document_data\", [])  # Retrieve document data in chunks\n",
        "    for idx, chunk in enumerate(document_chunks):\n",
        "        print(f\"Chunk {idx + 1}:\")\n",
        "        print(chunk)  # Display each chunk of the document\n",
        "        print(\"-\" * 50)  # Separator for clarity\n",
        "\n",
        "    # Chunked fetching is especially useful for large documents, as it prevents memory overload\n",
        "except Exception as e:\n",
        "    print(\"Error processing fetched data:\", str(e))"
      ],
      "metadata": {
        "id": "KuRlTLwF6Odx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Why we are fetching in Chunks\n",
        "1. Improved Performance\n",
        "2. Customizable Fetching\n",
        "3. Avoiding Memory Overload\n",
        "4. Partial Data Processing\n",
        "\n",
        "# Also, LLMs have a certain token limit, so we cannot exceed it."
      ],
      "metadata": {
        "id": "08FG16kuAbO_"
      }
    }
  ]
}