{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7Ebol+glb1M3U8kIWJp38",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sprayer1122/Groclake_Tutorials/blob/main/RAG_Tutorial_with_Groclake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Tutorial with Groclake\n",
        "Description:\n",
        "This complete, end-to-end tutorial demonstrates how to create an Agentic Retrieval-Augmented Generation (RAG) system using Groclake. The process involves managing documents in DataLake, generating vectors for documents, performing vector searches, enriching search results, and utilizing ModelLake to provide contextual, AI-assisted responses. Each step is designed to showcase the capabilities of Groclake in creating a fully functional Agentic RAG system.\n",
        "\n",
        "Groclake Documentation: https://plotch-ai.gitbook.io/groclake-by-plotch.ai\n",
        "\n",
        "Vectorcake is a vector centric infrastructure allowing developers to create embedding vectors quickly, store vectors and build useful RAG applications.\n",
        "\n",
        "Datalake is a data warehouse for storing various types structured and unstructured documents and records. Using Datalake, developers can store pdfs, word documents, excel sheets, google sheets, texts etc for RAG based applications.\n",
        "\n",
        "Modelake is an infrastructure pipe for LLM based operations like chat completions, language translations, automatic speech recognition, text to speech, speech to text and speech to speech operations"
      ],
      "metadata": {
        "id": "3i3YyNwXTKvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install the Required Library\n",
        "First, install the groclake library, which will be used for managing data, vectors, and models"
      ],
      "metadata": {
        "id": "t5QydZxVSEp1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhIfcr6k-vnS"
      },
      "outputs": [],
      "source": [
        "!pip install groclake"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Set Environment Variables\n",
        "Set up the API key and account ID for authenticating with the Groclake service. These are stored as environment variables to simplify access throughout the script."
      ],
      "metadata": {
        "id": "BJeU1xg4SOPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set API key and account ID\n",
        "GROCLAKE_API_KEY = 'your_groclake_api_key'\n",
        "GROCLAKE_ACCOUNT_ID = 'your_groclake_account_id'\n",
        "\n",
        "# Set them as environment variables\n",
        "os.environ['GROCLAKE_API_KEY'] = GROCLAKE_API_KEY\n",
        "os.environ['GROCLAKE_ACCOUNT_ID'] = GROCLAKE_ACCOUNT_ID\n",
        "\n",
        "print(\"Environment variables set successfully.\")\n"
      ],
      "metadata": {
        "id": "YBQDSTld-xeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Initialize VectorLake and DataLake\n",
        "Create instances of VectorLake and DataLake. These are core components for managing vectors and data"
      ],
      "metadata": {
        "id": "VkK5IYEhSY8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groclake.vectorlake import Vectorlake\n",
        "from groclake.datalake import Datalake\n",
        "from groclake.modellake import Modellake\n",
        "\n",
        "try:\n",
        "    # Initialize VectorLake\n",
        "    vectorlake = Vectorlake()\n",
        "    vector_create = vectorlake.create()\n",
        "    vectorlake_id = vector_create[\"vectorlake_id\"]\n",
        "    print(f\"VectorLake created with ID: {vectorlake_id}\")\n",
        "\n",
        "    # Initialize DataLake\n",
        "    datalake = Datalake()\n",
        "    datalake_create = datalake.create()\n",
        "    datalake_id = datalake_create[\"datalake_id\"]\n",
        "    print(f\"DataLake created with ID: {datalake_id}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during VectorLake or DataLake creation:\", str(e))\n"
      ],
      "metadata": {
        "id": "UQq3ZmEI-x-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Push a Document to DataLake\n",
        "Upload a document to DataLake for processing. The document in this case is accessed via a URL."
      ],
      "metadata": {
        "id": "egclsEj0She2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Prepare payload for pushing the document\n",
        "    payload_push = {\n",
        "        \"datalake_id\": datalake_id,\n",
        "        \"document_type\": \"url\",\n",
        "        \"document_data\": \"https://drive.google.com/uc?id=1PnGGUo9vpwyKpQe1lUW1N4An9l39xf9I\"   #example document of greenfield City\n",
        "    }\n",
        "\n",
        "    # Push the document\n",
        "    data_push = datalake.document_push(payload_push)\n",
        "    document_id = data_push.get(\"document_id\")\n",
        "\n",
        "    if not document_id:\n",
        "        raise ValueError(\"Document ID not found in the push response.\")\n",
        "\n",
        "    print(f\"Document pushed successfully. Document ID: {document_id}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while pushing document:\", str(e))\n"
      ],
      "metadata": {
        "id": "HhhlSOEd-5Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Fetch the Document from DataLake\n",
        "Retrieve the document in chunks for further processing."
      ],
      "metadata": {
        "id": "oiT079IdSkzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Prepare payload for fetching the document\n",
        "    payload_fetch = {\n",
        "        \"document_id\": document_id,\n",
        "        \"datalake_id\": datalake_id,\n",
        "        \"fetch_format\": \"chunk\",\n",
        "        \"chunk_size\": \"500\"\n",
        "    }\n",
        "\n",
        "    # Fetch the document\n",
        "    data_fetch = datalake.document_fetch(payload_fetch)\n",
        "    document_chunks = data_fetch.get(\"document_data\", [])\n",
        "\n",
        "    if not document_chunks:\n",
        "        raise ValueError(\"No document data found.\")\n",
        "\n",
        "    print(f\"Document fetched successfully. Total chunks: {len(document_chunks)}\")\n",
        "\n",
        "    # Print each chunk and its index\n",
        "    for index, chunk in enumerate(document_chunks):\n",
        "        print(f\"Chunk {index + 1}: {chunk}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while fetching document:\", str(e))\n"
      ],
      "metadata": {
        "id": "atuVw2qV_Ak1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Process and Push Document Chunks\n",
        "For each chunk, generate a vector representation and push it to VectorLake."
      ],
      "metadata": {
        "id": "CKtyW4XvSttH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    for idx, chunk in enumerate(document_chunks):\n",
        "        print(f\"Processing chunk {idx + 1}: {chunk}\")\n",
        "\n",
        "        # Generate vector for the chunk\n",
        "        vector_doc = vectorlake.generate(chunk)\n",
        "        vector_chunk = vector_doc.get(\"vector\")\n",
        "\n",
        "        if not vector_chunk:\n",
        "            raise ValueError(f\"Vector generation failed for chunk {idx + 1}.\")\n",
        "\n",
        "        # Prepare payload for pushing the vector\n",
        "        vectorlake_push_request = {\n",
        "            \"vector\": vector_chunk,\n",
        "            \"vectorlake_id\": vectorlake_id,\n",
        "            \"document_text\": chunk,\n",
        "            \"vector_type\": \"text\",\n",
        "            \"metadata\": {}\n",
        "        }\n",
        "\n",
        "        # Push vector to VectorLake\n",
        "        push_response = vectorlake.push(vectorlake_push_request)\n",
        "        print(f\"Push response for chunk {idx + 1}: {push_response}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while processing and pushing chunks:\", str(e))\n"
      ],
      "metadata": {
        "id": "xcmqcvbb_EV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Perform a Vector Search\n",
        "Search VectorLake using a query. Generate a vector for the search query and execute the search."
      ],
      "metadata": {
        "id": "sKpHbOeBSxUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Generate vector for the search query\n",
        "    search_query = \"Recylcing Rate\"\n",
        "    vector_search_data = vectorlake.generate(search_query)\n",
        "    search_vector = vector_search_data.get(\"vector\")\n",
        "\n",
        "    if not search_vector:\n",
        "        raise ValueError(\"Search vector generation failed.\")\n",
        "\n",
        "    # Prepare payload for the search\n",
        "    search_payload = {\n",
        "        \"vector\": search_vector,\n",
        "        \"vectorlake_id\": vectorlake_id,\n",
        "        \"vector_type\": \"text\",\n",
        "    }\n",
        "\n",
        "    # Perform the search\n",
        "    search_response = vectorlake.search(search_payload)\n",
        "    print(\"Search results:\", search_response)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error while performing vector search:\", str(e))\n"
      ],
      "metadata": {
        "id": "vPRuO86s_IeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Advanced Search and Enrich Context with ModelLake\n",
        "Use the search results to enrich a context and query ModelLake for additional insights."
      ],
      "metadata": {
        "id": "BWC0HnS6S0pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Generate vector for the search query\n",
        "    search_query = \"Tell me about Greenfield city its demographics and details\"\n",
        "    vector_search_data = vectorlake.generate(search_query)\n",
        "    search_vector = vector_search_data.get(\"vector\")\n",
        "\n",
        "    if not search_vector:\n",
        "        raise ValueError(\"Search vector generation failed.\")\n",
        "\n",
        "    # Prepare the vector search request with metadata\n",
        "    vectorlake_search_request = {\n",
        "        \"vector\": search_vector,\n",
        "        \"vector_type\": \"text\",\n",
        "        \"vector_document\": search_query,\n",
        "        \"metadata\": {\n",
        "            \"key\": \"value\"  # Include custom metadata as needed\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"VectorLake Search Request:\", vectorlake_search_request)\n",
        "\n",
        "    # Perform vector search in VectorLake\n",
        "    search_response = vectorlake.search(vectorlake_search_request)\n",
        "    print(\"Search Response:\", search_response)\n",
        "\n",
        "    # Extract search results from the response\n",
        "    search_results = search_response.get(\"results\", [])\n",
        "    if not search_results:\n",
        "        raise ValueError(\"No relevant search results found.\")\n",
        "\n",
        "    # Combine relevant vector documents into enriched context\n",
        "    enriched_context = []\n",
        "    token_count = 0\n",
        "\n",
        "    for result in search_results:\n",
        "        doc_content = result.get(\"vector_document\", \"\")\n",
        "        doc_tokens = len(doc_content.split())\n",
        "\n",
        "        if token_count + doc_tokens <= 1000:  # Adjust limit dynamically\n",
        "            enriched_context.append(doc_content)\n",
        "            token_count += doc_tokens\n",
        "        else:\n",
        "            break  # Stop when the token limit is reached\n",
        "\n",
        "    enriched_context = \" \".join(enriched_context)\n",
        "    print(\"Enriched Context:\", enriched_context)\n",
        "\n",
        "    # Construct the ModelLake query with enriched context\n",
        "    payload = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Using the following context from retrieved documents: {enriched_context}, \"\n",
        "                           f\"please provide a detailed explanation.\"\n",
        "            }\n",
        "        ],\n",
        "        \"token_size\": 3000\n",
        "    }\n",
        "\n",
        "    # Query ModelLake for a response\n",
        "    try:\n",
        "        chat_response = Modellake().chat_complete(payload)\n",
        "        # Extract the assistant's answer\n",
        "        answer = chat_response.get(\"answer\", \"No answer received from ModelLake.\")\n",
        "        print(\"Chat Answer:\", answer)\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred with ModelLake:\", str(e))\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during vector search or processing:\", str(e))\n"
      ],
      "metadata": {
        "id": "0AcqTfqcShgy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}